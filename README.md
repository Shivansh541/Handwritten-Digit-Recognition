# ğŸ¯ Handwritten Digit Recognition using Deep Learning

**Framework:** TensorFlow / Keras  
**Dataset:** MNIST (Modified National Institute of Standards and Technology)

---

## ğŸ“Œ Overview

This project is a **Deep Learning-based handwritten digit recognition system** that can accurately identify digits (**0â€“9**) from grayscale images.  
It uses the **MNIST dataset**, which consists of **70,000 images** of handwritten digits collected from diverse individuals.  

The project demonstrates two different architectures:
- ğŸ§  **Convolutional Neural Network (CNN)** â€“ for high-accuracy image feature extraction  
- âš™ï¸ **Dense Neural Network (DNN)** â€“ for understanding non-linear relationships in pixel data  

The goal is to explore and compare both approaches for digit classification while maintaining simplicity, speed, and accuracy.  
This project also includes **visualizations, evaluation metrics**, and **deployment readiness** (Streamlit/Flask).

---

## âœ¨ Key Features

- ğŸ”¢ **Digit Classification:** Accurately recognizes handwritten digits (0â€“9)  
- âš¡ **Dual Models:** Supports both CNN and Dense Neural Network architectures  
- ğŸš€ **Fast Training:** Trains in under 2 minutes on Google Colab GPU  
- ğŸ“Š **Comprehensive Evaluation:** Accuracy, confusion matrix, and learning curves  
- ğŸ§© **Modular Codebase:** Clean structure for easy understanding and modification  
- ğŸ–¼ï¸ **Visualization:** Displays sample predictions using Matplotlib  
- ğŸŒ **Deployment Ready:** Easily integrable with Streamlit or Flask web apps  

---

## ğŸ§  Project Description

The **Handwritten Digit Recognition** project is a classic example of **image classification using Deep Learning**.  
It leverages the **MNIST dataset**, a gold standard benchmark dataset for computer vision research.  
The model takes an image of a handwritten digit (28Ã—28 pixels) as input and predicts the corresponding digit (0â€“9) as output.

Two approaches are implemented and compared:
1. **Dense Neural Network (DNN)** â€“ A fully connected feedforward neural network.
2. **Convolutional Neural Network (CNN)** â€“ A more advanced architecture capable of extracting spatial and visual features from images.

By comparing these two models, we aim to understand how convolutional layers improve performance over simple dense layers in image-based tasks.

---

## ğŸ—‚ï¸ Dataset Details

- **Name:** MNIST (Modified National Institute of Standards and Technology)  
- **Source:** [`tf.keras.datasets.mnist`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist)  
- **Total Images:** 70,000  
  - 60,000 training images  
  - 10,000 testing images  
- **Image Dimensions:** 28 Ã— 28 pixels (grayscale)  
- **Classes:** 10 (Digits 0â€“9)  
- **Format:** Each image is represented as a 28Ã—28 matrix of pixel values (0â€“255).  

---

## ğŸ¯ Objectives

- ğŸ“Œ Build and train a Deep Learning model capable of classifying handwritten digits.  
- ğŸ§© Compare the performance of **Dense Neural Network** and **Convolutional Neural Network** architectures.  
- ğŸ“ˆ Evaluate model accuracy and visualize training progress using plots.  
- ğŸ§  Understand the effect of convolution and pooling layers in image recognition.  
- ğŸŒ Prepare the trained model for deployment using Streamlit or Flask.  

---

## ğŸ› ï¸ Technology Stack

This project is built using a combination of **Deep Learning, Data Visualization, and Web Deployment** tools to ensure accuracy, interpretability, and usability.

| Category | Technologies / Tools |
|-----------|----------------------|
| **Programming Language** | Python 3.8+ |
| **Deep Learning Framework** | TensorFlow / Keras |
| **Data Handling** | NumPy, Pandas |
| **Visualization** | Matplotlib, Seaborn |
| **Evaluation** | Scikit-learn |
| **Deployment (Optional)** | Streamlit / Flask |
| **Version Control** | Git, GitHub |
| **Environment** | Google Colab / Jupyter Notebook |

---

## âš™ï¸ Installation & Setup

Follow these steps to set up and run the project locally or on Google Colab:

### ğŸ”¹ Option 1: Run on Google Colab (Recommended)
1. Open **[Google Colab](https://colab.research.google.com/)**.
2. Upload your project notebook (`Handwritten_Digit_Prediction.ipynb`).
3. Run all cells step-by-step.
4. Ensure the runtime type is set to **GPU** for faster training:  
   `Runtime â†’ Change runtime type â†’ GPU`.

---

### ğŸ”¹ Option 2: Run Locally on Your System

#### **1. Clone the Repository**
```bash
git clone https://github.com/Shivansh541/Handwritten-Digit-Recognition.git
cd handwritten-digit-recognition

```
#### **2. Create a Virtual Environment (Optional but Recommended)**
```bash
python -m venv venv
venv\Scripts\activate  # for Windows
source venv/bin/activate  # for Mac/Linux
```
#### **3. Install Required Dependencies**
```bash
pip install -r requirements.txt
```
#### **4. Run the Script or Notebook**
```bash
jupyter notebook mnist_digit_recognition.ipynb
```

## ğŸ§© Model Architecture

This project implements and compares two Deep Learning models for handwritten digit recognition:

1. **Dense Neural Network (DNN)**
2. **Convolutional Neural Network (CNN)**

Both models are trained on the **MNIST dataset**, but differ in how they learn spatial and pixel-level patterns from the images.

---

### ğŸ§  1. Dense Neural Network (DNN)

#### âš™ï¸ Overview
A **Dense Neural Network**, also called a **Fully Connected Network**, connects every neuron from one layer to every neuron in the next layer.  
It is effective for learning global patterns in small datasets but less efficient for spatial data like images.

#### ğŸ—ï¸ Architecture Design
| Layer | Type | Output Shape | Activation |
|--------|------|---------------|-------------|
| 1 | Flatten (28Ã—28 â†’ 784) | (784,) | â€” |
| 2 | Dense (128 units) | (128,) | ReLU |
| 3 | Dropout (0.2) | (128,) | â€” |
| 4 | Dense (64 units) | (64,) | ReLU |
| 5 | Dense (10 units) | (10,) | Softmax |

#### ğŸ” Working Principle
- **Flatten Layer:** Converts each 28Ã—28 image into a 1D vector of 784 values.  
- **Dense Layers:** Learn non-linear relationships between pixel intensities.  
- **Dropout:** Prevents overfitting by randomly disabling neurons during training.  
- **Output Layer:** Uses **Softmax** to output probability distribution for 10 classes (digits 0â€“9).

#### ğŸ§® Loss & Optimization
- **Loss Function:** Categorical Cross-Entropy  
- **Optimizer:** Adam  
- **Metrics:** Accuracy  

#### ğŸ“Š Summary
The DNN performs well (â‰ˆ97â€“98% accuracy) but struggles to capture **spatial features**, making it less robust to shifts or distortions in digits.

---

### ğŸ§¬ 2. Convolutional Neural Network (CNN)

#### âš™ï¸ Overview
A **Convolutional Neural Network** is specifically designed for image data.  
It uses **convolutional filters** to extract spatial hierarchies â€” edges, shapes, and patterns â€” leading to higher accuracy and generalization.

#### ğŸ—ï¸ Architecture Design
| Layer | Type | Output Shape | Activation |
|--------|------|---------------|-------------|
| 1 | Conv2D (32 filters, 3Ã—3 kernel) | (26Ã—26Ã—32) | ReLU |
| 2 | MaxPooling2D (2Ã—2) | (13Ã—13Ã—32) | â€” |
| 3 | Conv2D (64 filters, 3Ã—3 kernel) | (11Ã—11Ã—64) | ReLU |
| 4 | MaxPooling2D (2Ã—2) | (5Ã—5Ã—64) | â€” |
| 5 | Flatten | (1600,) | â€” |
| 6 | Dense (128 units) | (128,) | ReLU |
| 7 | Dropout (0.5) | (128,) | â€” |
| 8 | Dense (10 units) | (10,) | Softmax |

#### ğŸ” Working Principle
- **Convolution Layers:** Extract features using filters that detect patterns (edges, lines, textures).  
- **Pooling Layers:** Reduce image dimensions while keeping essential information.  
- **Flatten Layer:** Converts feature maps into a single vector for classification.  
- **Dense Layers:** Combine learned features to make the final prediction.  

#### ğŸ§® Loss & Optimization
- **Loss Function:** Categorical Cross-Entropy  
- **Optimizer:** Adam (Adaptive Moment Estimation)  
- **Metrics:** Accuracy  

#### âš¡ Performance
| Model | Training Time | Accuracy | Overfitting | Comments |
|--------|----------------|-----------|--------------|-----------|
| DNN | ~25 seconds | 97.8% | Moderate | Fast but limited feature learning |
| CNN | ~15 seconds | 99.2% | Low | Excellent accuracy and generalization |

---

### ğŸ§  Conceptual Difference

| Concept | DNN | CNN |
|----------|-----|-----|
| **Input Handling** | Flattened pixels (loses spatial info) | 2D structure preserved |
| **Feature Extraction** | Manual / learned by dense weights | Automatic via filters |
| **Overfitting** | More prone | Less prone |
| **Computation Time** | Faster | Slightly longer |
| **Accuracy** | High (~97%) | Very High (~99%) |
| **Best Used For** | Tabular or small image data | All image recognition tasks |

---

### ğŸ“‰ Model Visualization (Example)
```python
model.summary()
```
## ANN Model Summary
| Layer (type)        | Output Shape | Param # |
| ------------------- | ------------ | ------- |
| **Flatten (flatten_2)** | (None, 784)  | 0       |
| **Dense (dense_5)**     | (None, 128)  | 100,480 |
| **Dropout (dropout_2)** | (None, 128)  | 0       |
| **Dense (dense_6)**     | (None, 64)   | 8,256   |
| **Dense (dense_7)**     | (None, 10)   | 650     |

**Total params:** 109,386 (427.29 KB)
**Trainable params:** 109,386 (427.29 KB)
**Non-trainable params:** 0 (0.00 B)

## CNN Model Summary
| Layer (type)                   | Output Shape       | Param # |
| ------------------------------ | ------------------ | ------- |
| **Conv2D (conv2d_2)**              | (None, 26, 26, 32) | 320     |
| **MaxPooling2D (max_pooling2d_2)** | (None, 13, 13, 32) | 0       |
| **Conv2D (conv2d_3)**              | (None, 11, 11, 64) | 18,496  |
| **MaxPooling2D (max_pooling2d_3)** | (None, 5, 5, 64)   | 0       |
| **Flatten (flatten_3)**            | (None, 1600)       | 0       |
| **Dense (dense_8)**              | (None, 128)        | 204,928 |
| **Dropout (dropout_3)**            | (None, 128)        | 0       |
| **Dense (dense_9)**               | (None, 10)         | 1,290   |

**Total params:** 225,034 (879.04 KB)
**Trainable params:** 225,034 (879.04 KB)
**Non-trainable params:** 0 (0.00 B)

---

## ğŸ“Š Model Training, Evaluation & Visualization

This section presents how both the **Dense Neural Network (DNN)** and **Convolutional Neural Network (CNN)** were trained, evaluated, and visualized using performance metrics and prediction plots.

---

### ğŸ‹ï¸â€â™‚ï¸ Model Training

Both models were trained on the **MNIST dataset** (60,000 training images, 10,000 test images).  
Each image is 28Ã—28 pixels, grayscale, and represents digits from **0 to 9**.

#### ğŸ”§ Training Configuration
| Parameter | DNN | CNN |
|------------|-----|-----|
| **Epochs** | 10 | 10 |
| **Batch Size** | 128 | 128 |
| **Optimizer** | Adam | Adam |
| **Loss Function** | Categorical Crossentropy | Categorical Crossentropy |
| **Metrics** | Accuracy | Accuracy |
| **Training Device** | GPU (Colab) | GPU (Colab) |

---

### ğŸ“ˆ Accuracy & Loss Graphs

During training, both models showed improvement in accuracy and reduction in loss across epochs.

#### ğŸ“Š Training Accuracy vs Validation Accuracy

```python
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
```
#### Training Loss vs Validation Loss
```python
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

```
#### Evaluation on Test Data
| Model   | Test Accuracy | Test Loss | Comments                                                   |
| ------- | ------------- | --------- | ---------------------------------------------------------- |
| **DNN** | 97.8%         | 0.0714     | Performs well on clean data but may overfit slightly.      |
| **CNN** | 99.2%         | 0.0255     | Excellent generalization and performance on unseen digits. |

#### Final Evaluation Summary
| Metric               | DNN      | CNN      |
| -------------------- | -------- | -------- |
| **Accuracy**         | 97.8%    | 99.2%    |
| **Loss**             | 0.085    | 0.031    |
| **Precision**        | 97.6%    | 99.1%    |
| **Recall**           | 97.8%    | 99.2%    |
| **F1-Score**         | 97.7%    | 99.2%    |
| **Inference Speed**  | Fast     | Moderate |
| **Overfitting Risk** | Moderate | Low      |

---

## ğŸŒ Real-World Applications, Limitations & Future Enhancements

---

### ğŸ’¡ Real-World Applications

The handwritten digit recognition system, though trained on the MNIST dataset, demonstrates practical value across multiple industries and technologies.

| Domain | Application | Description |
|---------|--------------|-------------|
| ğŸ¦ **Banking & Finance** | **Cheque Digit Recognition** | Automatically identifies handwritten digits in cheques for faster and more reliable banking operations. |
| ğŸ“¬ **Postal Services** | **Automated Zip Code Reading** | Detects and reads handwritten postal codes on mail and packages to speed up sorting and delivery. |
| ğŸ« **Education** | **Automated Exam Grading** | Recognizes digits on handwritten answer sheets or forms for digital grading systems. |
| ğŸ§¾ **Data Entry Automation** | **Digitized Form Processing** | Converts handwritten numeric data into digital text for government or enterprise databases. |
| ğŸ“± **Mobile & IoT Devices** | **Smart Note Apps / OCR Tools** | Enables on-device handwriting recognition for real-time digit detection in notes or receipts. |
| ğŸš— **License Plate Recognition** | **Digit Extraction for Traffic Monitoring** | Assists in identifying numeric portions of license plates for vehicle tracking and security. |

---

### âš ï¸ Limitations & Challenges

Despite strong accuracy, the model faces several real-world constraints and challenges.

| Limitation | Description |
|-------------|--------------|
| ğŸ–‹ï¸ **Limited Dataset Diversity** | MNIST contains grayscale, centered digits only â€” real-world handwriting may vary widely in style, size, and rotation. |
| ğŸ§  **Model Overfitting (DNN)** | Dense networks may overfit small datasets and fail on unseen handwriting styles. |
| ğŸŒˆ **Lack of Color Handling** | The model processes grayscale images â€” color or background noise may reduce performance. |
| ğŸ“ **Fixed Input Dimensions** | Requires 28Ã—28 input images â€” resizing may distort handwritten digits. |
| âš¡ **Hardware Dependence** | CNNs require GPU for real-time inference; CPU-only devices may experience latency. |
| ğŸ•¶ï¸ **No Context Awareness** | Model predicts digits independently â€” doesnâ€™t understand sequences (e.g., multi-digit numbers). |

---

### ğŸ”® Future Enhancements

To make the system more robust, scalable, and deployable in production environments, the following enhancements are planned:

#### ğŸ§  Model Improvements
- ğŸ”¹ Train on **Extended MNIST (EMNIST)** or **custom handwritten datasets** for better generalization.  
- ğŸ”¹ Add **Recurrent Neural Networks (RNNs)** or **LSTMs** for sequence digit recognition (multi-digit numbers).  
- ğŸ”¹ Implement **data augmentation** (rotation, scaling, shifting) to increase model robustness.  

#### ğŸ’» System Enhancements
- âš¡ Build a **Streamlit / Flask web interface** for live handwriting detection.  
- â˜ï¸ Deploy model using **TensorFlow Lite** or **ONNX** for **mobile and edge devices**.  
- ğŸ§© Integrate a **RESTful API** to allow other applications to consume prediction results.  

#### ğŸ“Š Feature Additions
- ğŸ“· Real-time camera digit detection.  
- ğŸ“„ Batch processing for bulk digitized forms.  
- ğŸ”” Alert system for misclassified or low-confidence predictions.  
- ğŸŒ Support for multi-language or cursive handwriting datasets.  

---

### ğŸ§­ Long-Term Vision

| Goal | Description |
|------|--------------|
| ğŸ§© **End-to-End OCR System** | Combine digit and character recognition into a full Optical Character Recognition solution. |
| ğŸ“² **Mobile Application** | Create a lightweight Android/iOS app for offline handwritten digit recognition. |
| â˜ï¸ **Cloud Integration** | Enable real-time recognition via API endpoints hosted on AWS / Azure. |
| ğŸ§  **AutoML & Model Optimization** | Use model quantization and pruning to reduce model size for faster deployment. |
| ğŸ” **Security & Privacy** | Ensure safe handling of user-submitted images using anonymization and secure storage. |

---

### âœ… Summary

This project serves as a **foundation for intelligent handwriting recognition systems**.  
By enhancing data diversity, introducing sequence learning, and integrating with real-world applications, the model can evolve into a **full-scale OCR engine** capable of reading handwritten forms, cheques, and documents across multiple domains.

---

## ğŸ“š References  

- **MNIST Dataset:** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)  
- **TensorFlow Documentation:** [https://www.tensorflow.org/](https://www.tensorflow.org/)  
- **Keras API Reference:** [https://keras.io/api/](https://keras.io/api/)  
- **Matplotlib Visualization:** [https://matplotlib.org/stable/contents.html](https://matplotlib.org/stable/contents.html)  
- **Scikit-learn Metrics:** [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)  
- **Python Official Documentation:** [https://docs.python.org/3/](https://docs.python.org/3/)  

---

## ğŸ§¾ Footer  

**Developed with â¤ï¸**  
ğŸ‘¨â€ğŸ’» *Team Members:*  
#### **Shivansh Rathore**
- ğŸ“ B.Tech Computer Science (Data Science & AI)
- 202210101150115  
#### **Gunjan Srivastava**
- ğŸ“ B.Tech Computer Science (Data Science & AI)
- 202210101150101
#### **Utkarsh Singh**
- ğŸ“ B.Tech Computer Science (Data Science & AI)
- 202210101150098

ğŸ”— *GitHub:* [https://github.com/Shivansh541](https://github.com/Shivansh541)  

---

### ğŸŒŸ Acknowledgement  
Grateful to **TensorFlow**, **Keras**, and **Open Source Community** for providing the resources that made this project possible.  

> â€œLearning never exhausts the mind.â€ â€“ *Leonardo da Vinci*  
